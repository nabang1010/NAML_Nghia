{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import swifter\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from os import path\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import csv\n",
    "import importlib\n",
    "import nltk\n",
    "from config_nghia import model_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "try :\n",
    "    config = getattr(importlib.import_module('config_nghia') , f\"{model_name}Config\")\n",
    "except AttributeError:\n",
    "    print(f\"{model_name} not included!\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'config_nghia.NRMSConfig'>\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install swifter\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_row(row,category2int,entities2int,word2int):\n",
    "        new_row = [row.news_id, \n",
    "                   category2int[row.category] if row.category in category2int else 0, \n",
    "                   category2int[row.subcategory] if row.subcategory in category2int else 0,\n",
    "                   [0] * config.num_words_title, [0] * config.num_words_abstract,  # map word from titile and abstract to int\n",
    "                   [0] * config.num_words_title, [0] * config.num_words_abstract]  # map entity from title and abstract to int\n",
    "        local_entity_map = {}\n",
    "        for e in json.loads(row.title_entities):\n",
    "            # print(e)\n",
    "            #SurfaceForms raw entity name of original text\n",
    "            if e['Confidence'] > config.entity_confidence_threshold and e['WikidataId'] in entities2int:\n",
    "                for x in ' '.join(e['SurfaceForms']).lower().split(): #surfaceformrs is the original text\n",
    "                    local_entity_map[x] = entities2int[e['WikidataId']] #convert to one entity\n",
    "        for e in json.loads(row.abstract_entities):\n",
    "            if e['Confidence'] > config.entity_confidence_threshold and e['WikidataId'] in entities2int:\n",
    "                for x in ' '.join(e['SurfaceForms']).lower().split():\n",
    "                    # print(x)\n",
    "                    local_entity_map[x] = entities2int[e['WikidataId']]\n",
    "        try:\n",
    "            #if w is entity\n",
    "            for i , w in enumerate(word_tokenize(row.title.lower())):\n",
    "                if w in word2int:\n",
    "                    # print(w)\n",
    "                    new_row[3][i] = word2int[w]\n",
    "                    if w in local_entity_map:\n",
    "                        new_row[5][i] = local_entity_map[w]\n",
    "        except IndexError:\n",
    "            pass\n",
    "        try:\n",
    "            # if n w is entity\n",
    "            for i , w in enumerate(word_tokenize(row.abstract.lower())):\n",
    "                if w in word2int:\n",
    "                    new_row[4][i] = word2int[w]\n",
    "                    if w in local_entity_map:\n",
    "                        # print(w)\n",
    "                        new_row[6][i] = local_entity_map[w]\n",
    "        except IndexError:\n",
    "            pass\n",
    "        return pd.Series(new_row , \n",
    "                         index = ['id' , 'category' , 'subcategory' , 'title' , 'abstract' , 'title_entities' , 'abstract_entities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict_to_csv(dict_data , columns ,file_name ):\n",
    "    pd.DataFrame(dict_data.items() , columns = columns).to_csv(file_name , sep  = '\\t', index = False)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from :  /workspace/nabang1010/LBA_NLP/Recommendation_System/DATA/dev_small/news.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a37d6cefcdd848a8a772d3ae400b4d57",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/1000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to :  ./save_process/news_processed.tsv\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>title_entities</th>\n",
       "      <th>abstract_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N55528</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>[1, 2, 3, 4, 5, 6, 7, 5, 8, 6, 0, 0, 9, 0, 0, ...</td>\n",
       "      <td>[10, 1, 0, 5, 0, 5, 8, 11, 12, 1, 13, 14, 15, ...</td>\n",
       "      <td>[0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N18955</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[0, 19, 20, 0, 0, 21, 1, 0, 22, 23, 24, 25, 0,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N61837</td>\n",
       "      <td>5</td>\n",
       "      <td>6</td>\n",
       "      <td>[1, 26, 19, 27, 22, 28, 29, 30, 1, 0, 19, 31, ...</td>\n",
       "      <td>[33, 0, 0, 0, 34, 35, 0, 19, 36, 37, 38, 1, 39...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N53526</td>\n",
       "      <td>3</td>\n",
       "      <td>7</td>\n",
       "      <td>[52, 44, 45, 53, 54, 18, 55, 22, 56, 57, 0, 58...</td>\n",
       "      <td>[52, 61, 62, 52, 44, 35, 63, 5, 8, 64, 45, 53,...</td>\n",
       "      <td>[0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N38324</td>\n",
       "      <td>3</td>\n",
       "      <td>4</td>\n",
       "      <td>[56, 42, 71, 72, 19, 73, 74, 5, 75, 42, 35, 76...</td>\n",
       "      <td>[77, 78, 0, 5, 79, 80, 22, 35, 81, 82, 83, 84,...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "      <td>[0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       id  category  subcategory  \\\n",
       "0  N55528         1            2   \n",
       "1  N18955         3            4   \n",
       "2  N61837         5            6   \n",
       "3  N53526         3            7   \n",
       "4  N38324         3            4   \n",
       "\n",
       "                                               title  \\\n",
       "0  [1, 2, 3, 4, 5, 6, 7, 5, 8, 6, 0, 0, 9, 0, 0, ...   \n",
       "1  [0, 19, 20, 0, 0, 21, 1, 0, 22, 23, 24, 25, 0,...   \n",
       "2  [1, 26, 19, 27, 22, 28, 29, 30, 1, 0, 19, 31, ...   \n",
       "3  [52, 44, 45, 53, 54, 18, 55, 22, 56, 57, 0, 58...   \n",
       "4  [56, 42, 71, 72, 19, 73, 74, 5, 75, 42, 35, 76...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  [10, 1, 0, 5, 0, 5, 8, 11, 12, 1, 13, 14, 15, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [33, 0, 0, 0, 34, 35, 0, 19, 36, 37, 38, 1, 39...   \n",
       "3  [52, 61, 62, 52, 44, 35, 63, 5, 8, 64, 45, 53,...   \n",
       "4  [77, 78, 0, 5, 79, 80, 22, 35, 81, 82, 83, 84,...   \n",
       "\n",
       "                                      title_entities  \\\n",
       "0  [0, 0, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 2, 0, 0, 0, ...   \n",
       "3  [0, 0, 0, 3, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...   \n",
       "\n",
       "                                   abstract_entities  \n",
       "0  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "1  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "2  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  \n",
       "3  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 3, 0, 0, 0, ...  \n",
       "4  [0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, ...  "
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# News ID\n",
    "# Category\n",
    "# SubCategory\n",
    "# Title\n",
    "# Abstract\n",
    "# URL\n",
    "# Title Entities (entities contained in the title of this news)\n",
    "# Abstract Entities (entites contained in the abstract of this news)\n",
    "def parse_news(sourse, target, category2int_path, word2int_path,\n",
    "               entity2int_path, mode):\n",
    "    ''' \n",
    "    parse news for training and testing\n",
    "    args :\n",
    "        sourse : path to news.tsv\n",
    "        target : path to save news_precessed.tst\n",
    "    if mode == 'train' :\n",
    "        category2int_path and word2int_path and entity2int_path is path to save\n",
    "    else mode == test\n",
    "        category2int_path and word2int_path  and entity2int path is path to load from\n",
    "    '''\n",
    "    print('load from : ', sourse)\n",
    "    news = pd.read_table(sourse,\n",
    "                         header=None,\n",
    "                         usecols=[0, 1, 2, 3, 4, 5, 6, 7],\n",
    "                         names=['news_id', 'category', 'subcategory', 'title', 'abstract', 'url', 'title_entities', 'abstract_entities'])\n",
    "    # check\n",
    "    news = news[:1000]\n",
    "    # drop []\n",
    "    news['title_entities'].fillna('[]', inplace=True)  # replace NaN with []\n",
    "    news['abstract_entities'].fillna('[]', inplace=True)  # replace NaN with []\n",
    "    news.fillna(' ', inplace=True)  # replace NaN with ''\n",
    "    # category2int = json.load(category2int_path)\n",
    "    # sub_category2int = json.load(sub_category2int_path)\n",
    "    if mode == 'train':\n",
    "        category2int = {}\n",
    "        word2int = {}\n",
    "        entities2int = {}\n",
    "        word2freq = {}\n",
    "        entity2freq = {}\n",
    "        \n",
    "        for row in news.itertuples(index=False):\n",
    "            # map category to int\n",
    "            if row.category not in category2int:\n",
    "                category2int[row.category] = len(category2int) + 1\n",
    "            if row.subcategory not in category2int:\n",
    "                category2int[row.subcategory] = len(category2int) + 1\n",
    "            # create dictionary frequency of word\n",
    "            for w in word_tokenize(row.title.lower()):\n",
    "                if w not in word2freq:\n",
    "                    word2freq[w] = +1\n",
    "                else:\n",
    "                    word2freq[w] += 1\n",
    "            for w in word_tokenize(row.abstract.lower()):\n",
    "                if w not in word2freq:\n",
    "                    word2freq[w] = +1\n",
    "                else:\n",
    "                    word2freq[w] += 1\n",
    "            # process entities\n",
    "            # read json file from title entity entities of member in title\n",
    "            for e in json.loads(row.title_entities):\n",
    "                times = len(e['OccurrenceOffsets']) * e['Confidence']\n",
    "                if e['WikidataId'] not in entity2freq:\n",
    "                    entity2freq[e['WikidataId']] = times\n",
    "                else:\n",
    "                    entity2freq[e['WikidataId']] += times\n",
    "            for e in json.loads(row.abstract_entities):\n",
    "                times = len(e['OccurrenceOffsets']) * e['Confidence']\n",
    "                if e['WikidataId'] not in entity2freq:\n",
    "                    entity2freq[e['WikidataId']] = times\n",
    "                else:\n",
    "                    entity2freq[e['WikidataId']] += times\n",
    "\n",
    "        for k, v in word2freq.items():\n",
    "            if v > config.word_freq_threshold:\n",
    "                word2int[k] = len(word2int) + 1 \n",
    "        for k, v in entity2freq.items():\n",
    "            if v > config.entity_freq_threshold:\n",
    "                entities2int[k] = len(entities2int) + 1 # chi don gian dung freq de dem so lan suat hien nhung khong thuc su dung neu freq >=1 thi map ra mot so khong trung voi mot tu nao khac\n",
    "        \n",
    "        #swifter for apply faster\n",
    "        \n",
    "        def func(x): return parse_row(x, category2int,entities2int,word2int)\n",
    "        parse_news = news.swifter.apply(func, axis=1)\n",
    "        # print(parse_news)\n",
    "        parse_news.to_csv(target, sep='\\t', index=False)\n",
    "\n",
    "        # save dictionary\n",
    "        convert_dict_to_csv(\n",
    "            category2int, ['category', 'id'], category2int_path)\n",
    "        convert_dict_to_csv(word2int, ['word', 'id'], word2int_path)\n",
    "        convert_dict_to_csv(entities2int, ['entity', 'id'], entity2int_path)\n",
    "        #modify numword only use another dataset\n",
    "        print('save to : ', target)\n",
    "    elif mode == 'test':\n",
    "        category2int = dict(pd.read_table(category2int_path).values.tolist())\n",
    "        word2int = dict(pd.read_table(word2int_path , na_filter = False).values.tolist())\n",
    "        entities2int = dict(pd.read_table(entity2int_path).values.tolist())\n",
    "        \n",
    "        parse_news = news.swifter.apply(lambda x : parse_row(x , category2int , word2int , entities2int) , axis = 1)\n",
    "        parse_news.to_csv(target , sep = '\\t' , index = False)\n",
    "        \n",
    "    else :\n",
    "        raise Exception('mode must be train or test')\n",
    "    return parse_news\n",
    "    \n",
    "\n",
    "# def parse_news(sourse , target, category2int_path , word2int_path ,\n",
    "#                entity2int_path , mode):\n",
    "category2int_path = './save_process/category2int.csv'\n",
    "word2int_path = './save_process/word2int.csv'\n",
    "entity2int_path = 'save_process/entity2int.csv'\n",
    "target = './save_process/news_processed.tsv'\n",
    "news_path = '/workspace/nabang1010/LBA_NLP/Recommendation_System/DATA/dev_small/news.tsv'\n",
    "temp = parse_news(news_path, target, category2int_path,\n",
    "                  word2int_path, entity2int_path, 'train')\n",
    "# print(temp.shape)\n",
    "temp.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>brands</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>queen</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>elizabeth</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           id\n",
       "word         \n",
       "the         1\n",
       "brands      2\n",
       "queen       3\n",
       "elizabeth   4\n",
       ",           5"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'./save_process/word_embedding.csv'\n",
    "word2int = pd.read_table(word2int_path , na_filter = False , index_col= 'word')\n",
    "word2int.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate of word missed in pretrained embedding: 0.0165\n"
     ]
    }
   ],
   "source": [
    "glove_path =  '/workspace/nabang1010/LBA_NLP/Recommendation_System/REPO/NRMS-Pytorch/word2vec/glove/glove.6B.50d.txt'\n",
    "def generate_word_embedding(sourse , target , word2int_path  , emb_dim  = 50):\n",
    "    ''' use glove to generate word embedding\n",
    "    args :\n",
    "        sourse: path to glove file\n",
    "        target: path to save word embedding\n",
    "        word2int_path: path to load word2int\n",
    "    '''\n",
    "    word2int = pd.read_table(word2int_path , na_filter = False , index_col= 'word')\n",
    "    # print(word2int)\n",
    "    word2vec = {} \n",
    "    source_embedding = pd.read_table(sourse , \n",
    "                                     index_col= 0 , \n",
    "                                     sep = ' ', \n",
    "                                     header = None,\n",
    "                                     quoting = csv.QUOTE_NONE , \n",
    "                                    #  names = range(config.word_embedding_dim), \n",
    "                                     names = range(emb_dim), \n",
    "                                     )\n",
    "    source_embedding.index.rename('word' , inplace = True) # set the index is word2\n",
    "    \n",
    "    merged = word2int.merge(source_embedding , \n",
    "                            how = 'inner' , \n",
    "                            right_on= 'word' ,\n",
    "                            left_on = 'word'\n",
    "                            )\n",
    "    merged.set_index('id' , inplace = True)\n",
    "    #process miss index\n",
    "    missed_index = np.setdiff1d(np.arange(len(word2int) +1) , # all index of word \n",
    "                                merged.index.values) #ouput miss index\n",
    "    missed_embedding = pd.DataFrame(data = np.random.normal(\n",
    "        size =( len(missed_index) , emb_dim)))\n",
    "    missed_embedding['id'] = missed_index\n",
    "    missed_embedding.set_index('id', inplace = True)\n",
    "    merged = pd.concat([merged , missed_embedding]).sort_index()\n",
    "    print(\n",
    "    f'Rate of word missed in pretrained embedding: {(len(missed_index)-1)/len(word2int):.4f}'\n",
    "    )\n",
    "    np.save(target , merged.values)\n",
    "    merged.head()\n",
    "    # return merged\n",
    "generate_word_embedding(glove_path , './save_process/word_embedding' , word2int_path)\n",
    "# df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from numpy import source\n",
    "\n",
    "\n",
    "def transform_entity_embedding(source, target, entity2int_path):\n",
    "    \"\"\"\n",
    "    Args:\n",
    "        source: path of embedding file\n",
    "        target: path of transformed embedding file in numpy format\n",
    "        entity2int_path\n",
    "    \"\"\"\n",
    "    entity_embedding = pd.read_table(source, header=None) # read embedding file\n",
    "    entity_embedding['vector'] = entity_embedding.iloc[:,\n",
    "                                                       1:101].values.tolist() # lay den 101 chieu\n",
    "    entity_embedding = entity_embedding[[0, 'vector'\n",
    "                                         ]].rename(columns={0: \"entity\"}) # reanme vector to entity\n",
    "\n",
    "    entity2int = pd.read_table(entity2int_path) # read dict map entity to int\n",
    "    merged_df = pd.merge(entity_embedding, entity2int,\n",
    "                         on='entity').sort_values('id') # map entity to embedding in embeding file\n",
    "    entity_embedding_transformed = np.random.normal(\n",
    "        size=(len(entity2int) + 1, config.entity_embedding_dim)) # random  entity for what not in file\n",
    "    for row in merged_df.itertuples(index=False):\n",
    "        entity_embedding_transformed[row.id] = row.vector # what not in file will be random\n",
    "        # print(entity_embedding_transformed[row.id])\n",
    "    np.save(target, entity_embedding_transformed)\n",
    "source = '/workspace/nabang1010/LBA_NLP/Recommendation_System/DATA/dev_small/entity_embedding.vec'\n",
    "target = './save_process/entity_embedding'\n",
    "entity2int_path = './save_process/entity2int.csv'\n",
    "transform_entity_embedding(source, target, entity2int_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "balance data: 1000it [00:00, 18812.08it/s]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>history</th>\n",
       "      <th>candidate_news</th>\n",
       "      <th>clicked</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>N55189 N46039 N51741 N53234 N11276 N264 N40716...</td>\n",
       "      <td>N31958 N34130 N48740</td>\n",
       "      <td>1 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>N58715 N32109 N51180 N33438 N54827 N28488 N611...</td>\n",
       "      <td>N23513 N31958 N46976</td>\n",
       "      <td>1 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2</td>\n",
       "      <td>N56253 N1150 N55189 N16233 N61704 N51706 N5303...</td>\n",
       "      <td>N5940 N42844 N5472</td>\n",
       "      <td>1 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>3</td>\n",
       "      <td>N63554 N49153 N28678 N23232 N43369 N58518 N444...</td>\n",
       "      <td>N15347 N45057 N24802</td>\n",
       "      <td>1 0 0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>4</td>\n",
       "      <td>N51692 N18285 N26015 N22679 N55556</td>\n",
       "      <td>N5940 N62365 N23513</td>\n",
       "      <td>1 0 0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id                                            history  \\\n",
       "0        0  N55189 N46039 N51741 N53234 N11276 N264 N40716...   \n",
       "1        1  N58715 N32109 N51180 N33438 N54827 N28488 N611...   \n",
       "2        2  N56253 N1150 N55189 N16233 N61704 N51706 N5303...   \n",
       "3        3  N63554 N49153 N28678 N23232 N43369 N58518 N444...   \n",
       "4        4                 N51692 N18285 N26015 N22679 N55556   \n",
       "\n",
       "         candidate_news clicked  \n",
       "0  N31958 N34130 N48740   1 0 0  \n",
       "1  N23513 N31958 N46976   1 0 0  \n",
       "2    N5940 N42844 N5472   1 0 0  \n",
       "3  N15347 N45057 N24802   1 0 0  \n",
       "4   N5940 N62365 N23513   1 0 0  "
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Impression ID. The ID of an impression.\n",
    "# User ID. The anonymous ID of a user.\n",
    "# Time. The impression time with format \"MM/DD/YYYY HH:MM:SS AM/PM\".\n",
    "# History. The news click history (ID list of clicked news) of this user before this impression. The clicked news articles are ordered by time.\n",
    "# Impressions. List of news displayed in this impression and user's click behaviors on them (1 for click and 0 for non-click). The orders of news in a impressions have been shuffled.\n",
    "\n",
    "\n",
    "\n",
    "def parse_behavior(target , source , user2int_path):\n",
    "    df = pd.read_table(source , header = None,\n",
    "                       names = ['impression_id' , 'user_id' , 'time' , 'history' , 'impressions'])\n",
    "    #check\n",
    "    df = df[:1000]\n",
    "    df.history.fillna(' ' , inplace = True)\n",
    "    df.impressions = df.impressions.apply(lambda x: x.split())\n",
    "    user2int = {userid : i for i  , userid in enumerate(df.user_id.unique()) }\n",
    "    pd.DataFrame(user2int.items() , columns = ['user', 'id']).to_csv(user2int_path ,sep = '\\t' ,  index = False)\n",
    "    df.user_id = df.user_id.apply(lambda x : user2int[x])\n",
    "\n",
    "    for row in tqdm(df.itertuples() , desc = 'balance data'):\n",
    "        # print(row.impressions)\n",
    "        positive = iter([i for i in row.impressions if i.endswith('1')])\n",
    "        # print(next(positive))\n",
    "        negative = [i for i in row.impressions if i.endswith('0')]\n",
    "        random.shuffle(negative)\n",
    "        negative = iter(negative)\n",
    "        # print(negative)\n",
    "        # negative = iter(random.shuffle([i for i in row.impressions if i.endswith('0')]))   \n",
    "        # luu y random.shuffle khong tra ve gia tri nao ma no shuffle luon cai list cho minh implca = true default  \n",
    "        pairs = []\n",
    "        try :\n",
    "            while True :\n",
    "                pair = [next(positive)]\n",
    "                for _ in range(config.negative_sampling_ratio):\n",
    "                    pair.append(next(negative))\n",
    "                pairs.append(pair)\n",
    "        except StopIteration:\n",
    "            pass\n",
    "        # print(row.Index)\n",
    "        df.at[row.Index , 'impressions'] = pairs\n",
    "        # print(pairs)\n",
    "    df = df.explode('impressions').dropna(subset = ['impressions']).reset_index(drop = True) # tach tung doan pairs chua 1 \n",
    "    # # df.impressions = df.impressions.str.split()\n",
    "    df[['candidate_news' , 'clicked']] = pd.DataFrame(\n",
    "        df.impressions.map(lambda x: (\n",
    "            ' '.join(e.split('-')[0] for e in x),\n",
    "            \n",
    "            (' '.join(e.split('-')[1] for e in x))\n",
    "                                     )).tolist()\n",
    "    )\n",
    "    df.to_csv(target , sep = '\\t' , index = False ,\n",
    "              columns= ['user_id' , 'history' , 'candidate_news' , 'clicked' ])\n",
    "    \n",
    "    return df\n",
    "behavior2int_path = './save_process/behavior2int.csv'\n",
    "source = '/workspace/nabang1010/LBA_NLP/Recommendation_System/DATA/dev_small/behaviors.tsv'\n",
    "target = './save_process/behaviors_processed.tsv'\n",
    "temp = parse_behavior(target , source , behavior2int_path)\n",
    "# temp.head()\n",
    "df = pd.read_table(target)\n",
    "df.head()\n",
    "\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('lba_gnn37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0acbc6efc8d49867015fd0566c44348b9068f0c9a1fa0d66885855ebd09c73fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import swifter\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from os import path\n",
    "import random\n",
    "from nltk.tokenize import word_tokenize\n",
    "import numpy as np\n",
    "import csv\n",
    "import importlib\n",
    "import nltk\n",
    "from config_nghia import model_name\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "try :\n",
    "    config = getattr(importlib.import_module('config_nghia') , f\"{model_name}Config\")\n",
    "except AttributeError:\n",
    "    print(f\"{model_name} not included!\")\n",
    "    exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'config_nghia.NRMSConfig'>\n"
     ]
    }
   ],
   "source": [
    "print(config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install swifter\n",
    "# nltk.download('punkt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "def parse_row(row,category2int,entities2int,word2int):\n",
    "        new_row = [row.news_id, \n",
    "                   category2int[row.category] if row.category in category2int else 0, \n",
    "                   category2int[row.subcategory] if row.subcategory in category2int else 0,\n",
    "                   [0] * config.num_words_title, [0] * config.num_words_abstract,  # map word from titile and abstract to int\n",
    "                   [0] * config.num_words_title, [0] * config.num_words_abstract]  # map entity from title and abstract to int\n",
    "        local_entity_map = {}\n",
    "        for e in json.loads(row.title_entities):\n",
    "            # print(e)\n",
    "            #SurfaceForms raw entity name of original text\n",
    "            if e['Confidence'] > config.entity_confidence_threshold and e['WikidataId'] in entities2int:\n",
    "                for x in ' '.join(e['SurfaceForms']).split():\n",
    "                    local_entity_map[x] = entities2int[e['WikidataId']] #convert to one entity\n",
    "        for e in json.loads(row.abstract_entities):\n",
    "            if e['Confidence'] > config.entity_confidence_threshold and e['WikidataId'] in entities2int:\n",
    "                for x in ' '.join(e['SurfaceForms']).split():\n",
    "                    local_entity_map[x] = entities2int[e['WikidataId']]\n",
    "        try:\n",
    "            #if w is entity\n",
    "            for i , w in enumerate(word_tokenize(row.title.lower())):\n",
    "                if w in word2int:\n",
    "                    new_row[3][i] = word2int[w]\n",
    "                    if w in local_entity_map:\n",
    "                        new_row[5][i] = local_entity_map[w]\n",
    "        except IndexError:\n",
    "            pass\n",
    "        try:\n",
    "            # if n w is entity\n",
    "            for i , w in enumerate(word_tokenize(row.abstract.lower())):\n",
    "                if w in word2int:\n",
    "                    new_row[4][i] = word2int[w]\n",
    "                    if w in local_entity_map:\n",
    "                        new_row[6][i] = local_entity_map[w]\n",
    "        except IndexError:\n",
    "            pass\n",
    "        return pd.Series(new_row , \n",
    "                         index = ['id' , 'category' , 'subcategory' , 'title' , 'abstract' , 'title_entities' , 'abstract_entities'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_dict_to_csv(dict_data , columns ,file_name ):\n",
    "    pd.DataFrame(dict_data.items() , columns = columns).to_csv(file_name , sep  = '\\t', index = False)\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "load from :  /workspace/nabang1010/LBA_NLP/Recommendation_System/DATA/dev_small/news.tsv\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f9db5e3fea364f60b53471232b7934bd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Pandas Apply:   0%|          | 0/50 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "save to :  ./save_process/news_processed.tsv\n",
      "(50, 8)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>news_id</th>\n",
       "      <th>category</th>\n",
       "      <th>subcategory</th>\n",
       "      <th>title</th>\n",
       "      <th>abstract</th>\n",
       "      <th>url</th>\n",
       "      <th>title_entities</th>\n",
       "      <th>abstract_entities</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>N55528</td>\n",
       "      <td>lifestyle</td>\n",
       "      <td>lifestyleroyals</td>\n",
       "      <td>The Brands Queen Elizabeth, Prince Charles, an...</td>\n",
       "      <td>Shop the notebooks, jackets, and more that the...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAGH0ET.html</td>\n",
       "      <td>[{\"Label\": \"Prince Philip, Duke of Edinburgh\",...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>N18955</td>\n",
       "      <td>health</td>\n",
       "      <td>medical</td>\n",
       "      <td>Dispose of unwanted prescription drugs during ...</td>\n",
       "      <td></td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAISxPN.html</td>\n",
       "      <td>[{\"Label\": \"Drug Enforcement Administration\", ...</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>N61837</td>\n",
       "      <td>news</td>\n",
       "      <td>newsworld</td>\n",
       "      <td>The Cost of Trump's Aid Freeze in the Trenches...</td>\n",
       "      <td>Lt. Ivan Molchanets peeked over a parapet of s...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAJgNsz.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>N53526</td>\n",
       "      <td>health</td>\n",
       "      <td>voices</td>\n",
       "      <td>I Was An NBA Wife. Here's How It Affected My M...</td>\n",
       "      <td>I felt like I was a fraud, and being an NBA wi...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AACk2N6.html</td>\n",
       "      <td>[]</td>\n",
       "      <td>[{\"Label\": \"National Basketball Association\", ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>N38324</td>\n",
       "      <td>health</td>\n",
       "      <td>medical</td>\n",
       "      <td>How to Get Rid of Skin Tags, According to a De...</td>\n",
       "      <td>They seem harmless, but there's a very good re...</td>\n",
       "      <td>https://assets.msn.com/labs/mind/AAAKEkt.html</td>\n",
       "      <td>[{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...</td>\n",
       "      <td>[{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  news_id   category      subcategory  \\\n",
       "0  N55528  lifestyle  lifestyleroyals   \n",
       "1  N18955     health          medical   \n",
       "2  N61837       news        newsworld   \n",
       "3  N53526     health           voices   \n",
       "4  N38324     health          medical   \n",
       "\n",
       "                                               title  \\\n",
       "0  The Brands Queen Elizabeth, Prince Charles, an...   \n",
       "1  Dispose of unwanted prescription drugs during ...   \n",
       "2  The Cost of Trump's Aid Freeze in the Trenches...   \n",
       "3  I Was An NBA Wife. Here's How It Affected My M...   \n",
       "4  How to Get Rid of Skin Tags, According to a De...   \n",
       "\n",
       "                                            abstract  \\\n",
       "0  Shop the notebooks, jackets, and more that the...   \n",
       "1                                                      \n",
       "2  Lt. Ivan Molchanets peeked over a parapet of s...   \n",
       "3  I felt like I was a fraud, and being an NBA wi...   \n",
       "4  They seem harmless, but there's a very good re...   \n",
       "\n",
       "                                             url  \\\n",
       "0  https://assets.msn.com/labs/mind/AAGH0ET.html   \n",
       "1  https://assets.msn.com/labs/mind/AAISxPN.html   \n",
       "2  https://assets.msn.com/labs/mind/AAJgNsz.html   \n",
       "3  https://assets.msn.com/labs/mind/AACk2N6.html   \n",
       "4  https://assets.msn.com/labs/mind/AAAKEkt.html   \n",
       "\n",
       "                                      title_entities  \\\n",
       "0  [{\"Label\": \"Prince Philip, Duke of Edinburgh\",...   \n",
       "1  [{\"Label\": \"Drug Enforcement Administration\", ...   \n",
       "2                                                 []   \n",
       "3                                                 []   \n",
       "4  [{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...   \n",
       "\n",
       "                                   abstract_entities  \n",
       "0                                                 []  \n",
       "1                                                 []  \n",
       "2  [{\"Label\": \"Ukraine\", \"Type\": \"G\", \"WikidataId...  \n",
       "3  [{\"Label\": \"National Basketball Association\", ...  \n",
       "4  [{\"Label\": \"Skin tag\", \"Type\": \"C\", \"WikidataI...  "
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# News ID\n",
    "# Category\n",
    "# SubCategory\n",
    "# Title\n",
    "# Abstract\n",
    "# URL\n",
    "# Title Entities (entities contained in the title of this news)\n",
    "# Abstract Entities (entites contained in the abstract of this news)\n",
    "def parse_news(sourse, target, category2int_path, word2int_path,\n",
    "               entity2int_path, mode):\n",
    "    ''' \n",
    "    parse news for training and testing\n",
    "    args :\n",
    "        sourse : path to news.tsv\n",
    "        target : path to save news_precessed.tst\n",
    "    if mode == 'train' :\n",
    "        category2int_path and word2int_path and entity2int_path is path to save\n",
    "    else mode == test\n",
    "        category2int_path and word2int_path  and entity2int path is path to load from\n",
    "    '''\n",
    "    print('load from : ', sourse)\n",
    "    news = pd.read_table(sourse,\n",
    "                         header=None,\n",
    "                         usecols=[0, 1, 2, 3, 4, 5, 6, 7],\n",
    "                         names=['news_id', 'category', 'subcategory', 'title', 'abstract', 'url', 'title_entities', 'abstract_entities'])\n",
    "    # check\n",
    "    news = news[:50]\n",
    "    # drop []\n",
    "    news['title_entities'].fillna('[]', inplace=True)  # replace NaN with []\n",
    "    news['abstract_entities'].fillna('[]', inplace=True)  # replace NaN with []\n",
    "    news.fillna(' ', inplace=True)  # replace NaN with ''\n",
    "    # category2int = json.load(category2int_path)\n",
    "    # sub_category2int = json.load(sub_category2int_path)\n",
    "    if mode == 'train':\n",
    "        category2int = {}\n",
    "        word2int = {}\n",
    "        entities2int = {}\n",
    "        word2freq = {}\n",
    "        entity2freq = {}\n",
    "        \n",
    "        for row in news.itertuples(index=False):\n",
    "            # map category to int\n",
    "            if row.category not in category2int:\n",
    "                category2int[row.category] = len(category2int) + 1\n",
    "            if row.subcategory not in category2int:\n",
    "                category2int[row.subcategory] = len(category2int) + 1\n",
    "            # create dictionary frequency of word\n",
    "            for w in word_tokenize(row.title.lower()):\n",
    "                if w not in word2freq:\n",
    "                    word2freq[w] = +1\n",
    "                else:\n",
    "                    word2freq[w] += 1\n",
    "            for w in word_tokenize(row.abstract.lower()):\n",
    "                if w not in word2freq:\n",
    "                    word2freq[w] = +1\n",
    "                else:\n",
    "                    word2freq[w] += 1\n",
    "            # process entities\n",
    "            # read json file from title entity entities of member in title\n",
    "            for e in json.loads(row.title_entities):\n",
    "                times = len(e['OccurrenceOffsets']) * e['Confidence']\n",
    "                if e['WikidataId'] not in entity2freq:\n",
    "                    entity2freq[e['WikidataId']] = times\n",
    "                else:\n",
    "                    entity2freq[e['WikidataId']] += times\n",
    "            for e in json.loads(row.abstract_entities):\n",
    "                times = len(e['OccurrenceOffsets']) * e['Confidence']\n",
    "                if e['WikidataId'] not in entity2freq:\n",
    "                    entity2freq[e['WikidataId']] = times\n",
    "                else:\n",
    "                    entity2freq[e['WikidataId']] += times\n",
    "\n",
    "        for k, v in word2freq.items():\n",
    "            if v > config.word_freq_threshold:\n",
    "                word2int[k] = len(word2int) + 1 \n",
    "        for k, v in entity2freq.items():\n",
    "            if v > config.entity_freq_threshold:\n",
    "                entities2int[k] = len(entities2int) + 1 # chi don gian dung freq de dem so lan suat hien nhung khong thuc su dung neu freq >=1 thi map ra mot so khong trung voi mot tu nao khac\n",
    "        \n",
    "        #swifter for apply faster\n",
    "        def func(x): return parse_row(x, category2int, word2int, entities2int)\n",
    "        parse_news = news.swifter.apply(func, axis=1)\n",
    "        parse_news = parse_news.to_csv(target, sep='\\t', index=False)\n",
    "\n",
    "        # save dictionary\n",
    "        convert_dict_to_csv(\n",
    "            category2int, ['category', 'id'], category2int_path)\n",
    "        convert_dict_to_csv(word2int, ['word', 'id'], word2int_path)\n",
    "        convert_dict_to_csv(entities2int, ['entity', 'id'], entity2int_path)\n",
    "        #modify numword only use another dataset\n",
    "        print('save to : ', target)\n",
    "    elif mode == 'test':\n",
    "        category2int = dict(pd.read_table(category2int_path).values.tolist())\n",
    "        word2int = dict(pd.read_table(word2int_path , na_filter = False).values.tolist())\n",
    "        entities2int = dict(pd.read_table(entity2int_path).values.tolist())\n",
    "        \n",
    "        parse_news = news.swifter.apply(lambda x : parse_row(x , category2int , word2int , entities2int) , axis = 1)\n",
    "        parse_news.to_csv(target , sep = '\\t' , index = False)\n",
    "        \n",
    "    else :\n",
    "        raise Exception('mode must be train or test')\n",
    "    return news\n",
    "    \n",
    "\n",
    "# def parse_news(sourse , target, category2int_path , word2int_path ,\n",
    "#                entity2int_path , mode):\n",
    "category2int_path = './save_process/category2int.csv'\n",
    "word2int_path = './save_process/word2int.csv'\n",
    "entity2int_path = 'save_process/entity2int.csv'\n",
    "target = './save_process/news_processed.tsv'\n",
    "news_path = '/workspace/nabang1010/LBA_NLP/Recommendation_System/DATA/dev_small/news.tsv'\n",
    "temp = parse_news(news_path, target, category2int_path,\n",
    "                  word2int_path, entity2int_path, 'train')\n",
    "print(temp.shape)\n",
    "temp.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>word</th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>the</th>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>,</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>prince</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>and</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>by</th>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        id\n",
       "word      \n",
       "the      1\n",
       ",        2\n",
       "prince   3\n",
       "and      4\n",
       "by       5"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "'./save_process/word_embedding.csv'\n",
    "word2int = pd.read_table(word2int_path , na_filter = False , index_col= 'word')\n",
    "word2int.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Rate of word missed in pretrained embedding: 0.0037313432835820895\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>40</th>\n",
       "      <th>41</th>\n",
       "      <th>42</th>\n",
       "      <th>43</th>\n",
       "      <th>44</th>\n",
       "      <th>45</th>\n",
       "      <th>46</th>\n",
       "      <th>47</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-0.233005</td>\n",
       "      <td>0.742352</td>\n",
       "      <td>0.716705</td>\n",
       "      <td>1.464897</td>\n",
       "      <td>1.441201</td>\n",
       "      <td>1.518781</td>\n",
       "      <td>-0.862830</td>\n",
       "      <td>-0.270792</td>\n",
       "      <td>0.535216</td>\n",
       "      <td>0.168893</td>\n",
       "      <td>...</td>\n",
       "      <td>1.370916</td>\n",
       "      <td>-1.104349</td>\n",
       "      <td>2.016391</td>\n",
       "      <td>-0.805990</td>\n",
       "      <td>-0.415807</td>\n",
       "      <td>-0.427005</td>\n",
       "      <td>-0.301433</td>\n",
       "      <td>0.12219</td>\n",
       "      <td>-0.015293</td>\n",
       "      <td>0.410114</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.418000</td>\n",
       "      <td>0.249680</td>\n",
       "      <td>-0.412420</td>\n",
       "      <td>0.121700</td>\n",
       "      <td>0.345270</td>\n",
       "      <td>-0.044457</td>\n",
       "      <td>-0.496880</td>\n",
       "      <td>-0.178620</td>\n",
       "      <td>-0.000660</td>\n",
       "      <td>-0.656600</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.298710</td>\n",
       "      <td>-0.157490</td>\n",
       "      <td>-0.347580</td>\n",
       "      <td>-0.045637</td>\n",
       "      <td>-0.442510</td>\n",
       "      <td>0.187850</td>\n",
       "      <td>0.002785</td>\n",
       "      <td>-0.18411</td>\n",
       "      <td>-0.115140</td>\n",
       "      <td>-0.785810</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.013441</td>\n",
       "      <td>0.236820</td>\n",
       "      <td>-0.168990</td>\n",
       "      <td>0.409510</td>\n",
       "      <td>0.638120</td>\n",
       "      <td>0.477090</td>\n",
       "      <td>-0.428520</td>\n",
       "      <td>-0.556410</td>\n",
       "      <td>-0.364000</td>\n",
       "      <td>-0.239380</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.080262</td>\n",
       "      <td>0.630030</td>\n",
       "      <td>0.321110</td>\n",
       "      <td>-0.467650</td>\n",
       "      <td>0.227860</td>\n",
       "      <td>0.360340</td>\n",
       "      <td>-0.378180</td>\n",
       "      <td>-0.56657</td>\n",
       "      <td>0.044691</td>\n",
       "      <td>0.303920</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.988460</td>\n",
       "      <td>1.453500</td>\n",
       "      <td>-0.530810</td>\n",
       "      <td>0.105090</td>\n",
       "      <td>0.840580</td>\n",
       "      <td>0.140180</td>\n",
       "      <td>0.066562</td>\n",
       "      <td>1.334100</td>\n",
       "      <td>-0.758130</td>\n",
       "      <td>-0.352230</td>\n",
       "      <td>...</td>\n",
       "      <td>0.343310</td>\n",
       "      <td>1.183600</td>\n",
       "      <td>-0.371970</td>\n",
       "      <td>-1.106900</td>\n",
       "      <td>0.000128</td>\n",
       "      <td>-0.182020</td>\n",
       "      <td>-1.369600</td>\n",
       "      <td>-1.49700</td>\n",
       "      <td>0.406180</td>\n",
       "      <td>-0.424450</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.268180</td>\n",
       "      <td>0.143460</td>\n",
       "      <td>-0.278770</td>\n",
       "      <td>0.016257</td>\n",
       "      <td>0.113840</td>\n",
       "      <td>0.699230</td>\n",
       "      <td>-0.513320</td>\n",
       "      <td>-0.473680</td>\n",
       "      <td>-0.330750</td>\n",
       "      <td>-0.138340</td>\n",
       "      <td>...</td>\n",
       "      <td>-0.069043</td>\n",
       "      <td>0.368850</td>\n",
       "      <td>0.251680</td>\n",
       "      <td>-0.245170</td>\n",
       "      <td>0.253810</td>\n",
       "      <td>0.136700</td>\n",
       "      <td>-0.311780</td>\n",
       "      <td>-0.63210</td>\n",
       "      <td>-0.250280</td>\n",
       "      <td>-0.380970</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 50 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          0         1         2         3         4         5         6   \\\n",
       "id                                                                         \n",
       "0  -0.233005  0.742352  0.716705  1.464897  1.441201  1.518781 -0.862830   \n",
       "1   0.418000  0.249680 -0.412420  0.121700  0.345270 -0.044457 -0.496880   \n",
       "2   0.013441  0.236820 -0.168990  0.409510  0.638120  0.477090 -0.428520   \n",
       "3   0.988460  1.453500 -0.530810  0.105090  0.840580  0.140180  0.066562   \n",
       "4   0.268180  0.143460 -0.278770  0.016257  0.113840  0.699230 -0.513320   \n",
       "\n",
       "          7         8         9   ...        40        41        42        43  \\\n",
       "id                                ...                                           \n",
       "0  -0.270792  0.535216  0.168893  ...  1.370916 -1.104349  2.016391 -0.805990   \n",
       "1  -0.178620 -0.000660 -0.656600  ... -0.298710 -0.157490 -0.347580 -0.045637   \n",
       "2  -0.556410 -0.364000 -0.239380  ... -0.080262  0.630030  0.321110 -0.467650   \n",
       "3   1.334100 -0.758130 -0.352230  ...  0.343310  1.183600 -0.371970 -1.106900   \n",
       "4  -0.473680 -0.330750 -0.138340  ... -0.069043  0.368850  0.251680 -0.245170   \n",
       "\n",
       "          44        45        46       47        48        49  \n",
       "id                                                             \n",
       "0  -0.415807 -0.427005 -0.301433  0.12219 -0.015293  0.410114  \n",
       "1  -0.442510  0.187850  0.002785 -0.18411 -0.115140 -0.785810  \n",
       "2   0.227860  0.360340 -0.378180 -0.56657  0.044691  0.303920  \n",
       "3   0.000128 -0.182020 -1.369600 -1.49700  0.406180 -0.424450  \n",
       "4   0.253810  0.136700 -0.311780 -0.63210 -0.250280 -0.380970  \n",
       "\n",
       "[5 rows x 50 columns]"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "glove_path =  '/workspace/nabang1010/LBA_NLP/Recommendation_System/REPO/NRMS-Pytorch/word2vec/glove/glove.6B.50d.txt'\n",
    "def generate_word_embedding(sourse , target , word2int_path  , emb_dim  = 50):\n",
    "    ''' use glove to generate word embedding\n",
    "    args :\n",
    "        sourse: path to glove file\n",
    "        target: path to save word embedding\n",
    "        word2int_path: path to load word2int\n",
    "    '''\n",
    "    word2int = pd.read_table(word2int_path , na_filter = False , index_col= 'word')\n",
    "    # print(word2int)\n",
    "    word2vec = {} \n",
    "    source_embedding = pd.read_table(sourse , \n",
    "                                     index_col= 0 , \n",
    "                                     sep = ' ', \n",
    "                                     header = None,\n",
    "                                     quoting = csv.QUOTE_NONE , \n",
    "                                    #  names = range(config.word_embedding_dim), \n",
    "                                     names = range(emb_dim), \n",
    "                                     )\n",
    "    source_embedding.index.rename('word' , inplace = True) # set the index is word2\n",
    "    \n",
    "    merged = word2int.merge(source_embedding , \n",
    "                            how = 'inner' , \n",
    "                            right_on= 'word' ,\n",
    "                            left_on = 'word'\n",
    "                            )\n",
    "    merged.set_index('id' , inplace = True)\n",
    "    #process miss index\n",
    "    missed_index = np.setdiff1d(np.arange(len(word2int) +1) , # all index of word \n",
    "                                merged.index.values) #ouput miss index\n",
    "    missed_embedding = pd.DataFrame(data = np.random.normal(\n",
    "        size =( len(missed_index) , emb_dim)))\n",
    "    missed_embedding['id'] = missed_index\n",
    "    missed_embedding.set_index('id', inplace = True)\n",
    "    merged = pd.concat([merged , missed_embedding]).sort_index()\n",
    "    print(\n",
    "    f'Rate of word missed in pretrained embedding: {(len(missed_index)-1)/len(word2int):.4f}'\n",
    "    )\n",
    "    np.save(target , merged.values)\n",
    "    # return merged\n",
    "df = generate_word_embedding(glove_path , './save_process/word_embedding.csv' , word2int_path)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.7.13 ('lba_gnn37')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "0acbc6efc8d49867015fd0566c44348b9068f0c9a1fa0d66885855ebd09c73fd"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
